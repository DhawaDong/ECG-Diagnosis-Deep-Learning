{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhawaDong/ECG-Diagnosis-Deep-Learning/blob/main/BiLSTM_ECG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFzD3lNPXbJN"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import math\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6jC0GYrPV4p",
        "outputId": "c2ce8199-88df-4758-c740-34c07fa7c26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc1t-rrbX9Ju"
      },
      "outputs": [],
      "source": [
        "number_of_classes = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-M9z8IyZl2p"
      },
      "outputs": [],
      "source": [
        "def discritize(x):\n",
        "    answer = np.zeros((np.shape(x)[0]))\n",
        "    for i in range(np.shape(x)[0]):\n",
        "        max_value = max(x[i, :])\n",
        "        max_index = list(x[i, :]).index(max_value)\n",
        "        answer[i] = max_index\n",
        "    return answer.astype(np.int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8GCJbQGzZl5u",
        "outputId": "774d6a9a-73cb-4ad1-9a49-2d3c868dd2fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mypath = '/content/drive/MyDrive/training2017/'\n",
        "onlyfiles = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f[0] == 'A')]\n",
        "bats = [f for f in onlyfiles if f[7] == 'm']\n",
        "mats = [f for f in bats if (np.shape(sio.loadmat(mypath + f)['val'])[1] >= 9000)]\n",
        "SampleSize = np.shape(sio.loadmat(mypath + mats[0])['val'])[1]\n",
        "\n",
        "SampleSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4vg2VD3cK86j"
      },
      "outputs": [],
      "source": [
        "SampleSize = 9000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cybhHaVJB_Ex"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.utils import resample\n",
        "\n",
        "Train_data = pd.read_csv(mypath + 'REFERENCE.csv', sep=',', header=None, names=None)\n",
        "df_1 = Train_data[Train_data[1]=='A']\n",
        "df_2 = Train_data[Train_data[1]=='N']\n",
        "df_3 = Train_data[Train_data[1]=='O']\n",
        "df_4 = Train_data[Train_data[1]=='~']\n",
        "\n",
        "df_1_upsample = resample(df_1, replace=True, n_samples=5500, random_state=123)\n",
        "df_2_upsample = resample(df_2, replace=True, n_samples=5500, random_state=124)\n",
        "df_3_upsample = resample(df_3, replace=True, n_samples=5500, random_state=125)\n",
        "df_4_upsample = resample(df_4, replace=True, n_samples=8500, random_state=126)\n",
        "\n",
        "training_data = pd.concat([df_1_upsample, df_2_upsample, df_3_upsample, df_4_upsample])\n",
        "TrainDataSuffled = shuffle(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iZHw88EUCknS"
      },
      "outputs": [],
      "source": [
        "FileList = list(TrainDataSuffled[0] + '.mat')\n",
        "#len(FileList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aCdxEQ1aP0ae",
        "outputId": "2277f332-651e-4b66-9533-00d916f0348b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(738, 2) (5050, 2) (2456, 2) (284, 2)\n"
          ]
        }
      ],
      "source": [
        "print(df_1.shape, df_2.shape, df_3.shape, df_4.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GvQ16BT9Tcxi",
        "outputId": "eaf86b1d-2420-4d89-f5d3-fb766540e87c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5939"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DelRows = []\n",
        "for i in range(len(FileList)):\n",
        "  #print(i)\n",
        "  if (sio.loadmat(mypath + FileList[i])['val']).shape[1]<9000:\n",
        "    DelRows.append(FileList[i])\n",
        "\n",
        "len(DelRows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7ubOX1vftNtn",
        "outputId": "c000d320-6ee5-4273-8860-a2e9083dd219"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19061"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for j in range(len(DelRows)):\n",
        "  TrainDataSuffled.drop(TrainDataSuffled.loc[TrainDataSuffled[0] == DelRows[j][0:6]].index, inplace = True)\n",
        "\n",
        "NewFileList = list(TrainDataSuffled[0]+'.mat')\n",
        "len(NewFileList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lUGpBHMxHgJZ",
        "outputId": "11e89105-9276-4962-d3c6-3b10782a01f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19061"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(NewFileList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s2eYfSBuTX3E"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(NewFileList), SampleSize))\n",
        "for i in range(len(NewFileList)):\n",
        "    X[i, :] = sio.loadmat(mypath + NewFileList[i])['val'][0, :9000]\n",
        "    #print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EV1EILCgJXe1",
        "outputId": "c8485d67-dc97-4030-ee19-17b5500a9c2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19061, 9000)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2lRx8JP7y-a4",
        "outputId": "fa1468be-3dd7-4dbf-8038-4064392f56a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(19061, 19061)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(NewFileList), TrainDataSuffled.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "38RbBfMTZl80"
      },
      "outputs": [],
      "source": [
        "target_train = np.zeros((len(NewFileList), 1))\n",
        "#TrainDataSuffled  NewFileList\n",
        "#Train_data = pd.read_csv(mypath + 'REFERENCE.csv', sep=',', header=None, names=None)\n",
        "for i in range(len(NewFileList)):\n",
        "    if Train_data.loc[Train_data[0] == NewFileList[i][:6], 1].values == 'N':\n",
        "        target_train[i] = 0\n",
        "    elif Train_data.loc[Train_data[0] == NewFileList[i][:6], 1].values == 'A':\n",
        "        target_train[i] = 1\n",
        "    elif Train_data.loc[Train_data[0] == NewFileList[i][:6], 1].values == 'O':\n",
        "        target_train[i] = 2\n",
        "    else:\n",
        "        target_train[i] = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_Zey3ZVsZmAB",
        "outputId": "654ee364-f696-4bf9-e540-83d426279775"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-804af812e2c2>:4: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dummy[int(target_train[i])] = 1\n"
          ]
        }
      ],
      "source": [
        "Label_set = np.zeros((len(NewFileList), number_of_classes))\n",
        "for i in range(np.shape(target_train)[0]):\n",
        "    dummy = np.zeros((number_of_classes))\n",
        "    dummy[int(target_train[i])] = 1\n",
        "    Label_set[i, :] = dummy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ROXwYJihZmCz"
      },
      "outputs": [],
      "source": [
        "train_len = 0.8 #0.75\n",
        "X_train = X[:int(train_len*len(NewFileList)), :]\n",
        "Y_train = Label_set[:int(train_len*len(NewFileList)), :]\n",
        "X_val = X[int(train_len*len(NewFileList)):, :]\n",
        "Y_val = Label_set[int(train_len*len(NewFileList)):, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "31-d9eBNaLYr"
      },
      "outputs": [],
      "source": [
        "X_train = numpy.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_val = numpy.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QSMgdALAYsIO",
        "outputId": "d3725efa-b47a-448b-da68-5a65af3cd3e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(15248, 1, 9000) (3813, 1, 9000) 9000\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape, X_val.shape, SampleSize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TlO6vvME52XA",
        "outputId": "5c2c7a58-7c89-4696-8120-77ab0bbddac3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3931., 3756., 4070., 3491.])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OH5Cy8Dbf2YC",
        "outputId": "dc7c8ba6-8a52-4045-8ea4-2b80a5770d97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 996.,  944., 1007.,  866.])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJADW35C7jGf"
      },
      "outputs": [],
      "source": [
        "#Model Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RIn_cxEzC3zp",
        "outputId": "1a74418d-a301-4c44-8c9b-d69c6b9e1a56"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, mode='auto')\\nadam = keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.001)\\nadam = Adam(learning_rate=0.00001)\\n\\nmodel.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy']) #categorical_crossentropy\\n\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Deep BiLSTM Model\n",
        "batch_size = 64\n",
        "x_train = X_train,\n",
        "y_train = Y_train\n",
        "x_val = X_val,\n",
        "y_val = Y_val\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(1024, return_sequences=True), input_shape=(1, SampleSize)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "#output layer\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "initial_learning_rate = 0.00001\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, mode='auto')\n",
        "adam = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "\n",
        "# old version\n",
        "'''\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, mode='auto')\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.001)\n",
        "adam = Adam(learning_rate=0.00001)\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy']) #categorical_crossentropy\n",
        "'''\n",
        "# end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1_GEKuoEaLcW",
        "outputId": "c984f295-706a-4871-9a2a-a7253b4d5206"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nbatch_size = 64\\nx_train = np.transpose(X_train, (0, 2, 1))\\ny_train = Y_train\\n\\nx_val = np.transpose(X_val, (0, 2, 1))\\ny_val = Y_val\\n\\nmodel = Sequential()\\n# Three Convolutional Layers\\nmodel.add(Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(SampleSize, 1)))\\nmodel.add(MaxPooling1D(pool_size=2))\\n\\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\\nmodel.add(MaxPooling1D(pool_size=2))\\n\\nmodel.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\\nmodel.add(MaxPooling1D(pool_size=2))\\n\\n# Five BLSTM Layers\\n#model.add(Bidirectional(LSTM(units=1024, return_sequences=True)))\\n#model.add(Dropout(0.2))\\n\\n# Four BLSTM Layers\\nmodel.add(Bidirectional(LSTM(units=512, return_sequences=True)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Bidirectional(LSTM(units=256, return_sequences=True)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=True)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Bidirectional(LSTM(units=64)))\\n\\nmodel.add(Dense(64, activation='relu'))\\n#model.add(BatchNormalization())\\n\\nmodel.add(Dense(32, activation='relu'))\\n#model.add(BatchNormalization())\\n\\n# Output Layer\\nmodel.add(Dense(number_of_classes, activation='softmax'))\\n\\nearly_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, mode='auto')\\nadam = keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.001)\\n\\nmodel.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy']) #categorical_crossentropy\\n\""
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#CNN-BiLSTM Model\n",
        "'''\n",
        "batch_size = 64\n",
        "x_train = np.transpose(X_train, (0, 2, 1))\n",
        "y_train = Y_train\n",
        "\n",
        "x_val = np.transpose(X_val, (0, 2, 1))\n",
        "y_val = Y_val\n",
        "\n",
        "model = Sequential()\n",
        "# Three Convolutional Layers\n",
        "model.add(Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(SampleSize, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Five BLSTM Layers\n",
        "#model.add(Bidirectional(LSTM(units=1024, return_sequences=True)))\n",
        "#model.add(Dropout(0.2))\n",
        "\n",
        "# Four BLSTM Layers\n",
        "model.add(Bidirectional(LSTM(units=512, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Bidirectional(LSTM(units=256, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Bidirectional(LSTM(units=64)))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, mode='auto')\n",
        "adam = keras.optimizers.Adam(learning_rate = 0.00001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.001)\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = adam, metrics = ['accuracy']) #categorical_crossentropy\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QgutQo3BH7p_",
        "outputId": "513aff4d-57f1-41f1-fe44-daee590a2a2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, 1, 2048)           82124800  \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1, 2048)           0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 1, 1024)           10489856  \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1, 1024)           0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 1, 512)            2623488   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1, 512)            0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 1, 256)            656384    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1, 256)            0         \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 128)               164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4)                 132       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96069348 (366.48 MB)\n",
            "Trainable params: 96069348 (366.48 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "17unXcKsZmGD",
        "outputId": "22e08e8c-1507-4aae-a62f-b1fc14685588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "239/239 - 516s - loss: 1.3859 - accuracy: 0.2760 - val_loss: 1.3850 - val_accuracy: 0.2759 - 516s/epoch - 2s/step\n",
            "Epoch 2/20\n",
            "239/239 - 507s - loss: 1.3835 - accuracy: 0.3140 - val_loss: 1.3797 - val_accuracy: 0.4448 - 507s/epoch - 2s/step\n",
            "Epoch 3/20\n",
            "239/239 - 498s - loss: 1.3684 - accuracy: 0.5127 - val_loss: 1.3449 - val_accuracy: 0.6378 - 498s/epoch - 2s/step\n",
            "Epoch 4/20\n",
            "239/239 - 494s - loss: 1.2317 - accuracy: 0.5675 - val_loss: 1.0816 - val_accuracy: 0.5340 - 494s/epoch - 2s/step\n",
            "Epoch 5/20\n",
            "239/239 - 494s - loss: 1.0134 - accuracy: 0.5381 - val_loss: 0.9695 - val_accuracy: 0.5799 - 494s/epoch - 2s/step\n",
            "Epoch 6/20\n",
            "239/239 - 494s - loss: 0.8907 - accuracy: 0.6859 - val_loss: 0.8528 - val_accuracy: 0.7346 - 494s/epoch - 2s/step\n",
            "Epoch 7/20\n",
            "239/239 - 492s - loss: 0.7561 - accuracy: 0.7554 - val_loss: 0.7400 - val_accuracy: 0.7299 - 492s/epoch - 2s/step\n",
            "Epoch 8/20\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train, epochs=20, batch_size=batch_size,\n",
        "                    validation_data=(x_val, y_val), verbose=2, shuffle=False,\n",
        "                    callbacks=[early_stopping])\n",
        "#model.save('Keras_models/LSTM_ECG_Model_' + str(i) + '_' + str(j) + '_' + str() + '.h5')\n",
        "predictions = model.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhB_hiUYaabD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, recall_score, f1_score)\n",
        "Y_actual = discritize(y_val)\n",
        "Y_predict = discritize(predictions)\n",
        "\n",
        "Accuracy = accuracy_score(Y_actual, Y_predict)\n",
        "Precision = precision_score(Y_actual, Y_predict, average='weighted')\n",
        "Recall = recall_score(Y_actual, Y_predict, average='weighted')\n",
        "F1score = f1_score(Y_actual, Y_predict, average='weighted')\n",
        "\n",
        "print(Accuracy, Precision, Recall, F1score)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOUoUsPHXyMh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming discritize is a function that converts continuous/probability predictions to discrete classes\n",
        "def discritize(predictions, threshold=0.5):\n",
        "    #return np.argmax(predictions, axis=1)\n",
        "    return np.array([[1 if pred >= threshold else 0 for pred in row] for row in predictions])\n",
        "\n",
        "# Assuming y_val and predictions are available\n",
        "Y_actual = discritize(y_val)\n",
        "Y_predict = discritize(predictions)\n",
        "\n",
        "print(Y_actual)\n",
        "print(Y_predict)\n",
        "\n",
        "# Calculate classification metrics\n",
        "Accuracy = accuracy_score(Y_actual, Y_predict)\n",
        "Precision = precision_score(Y_actual, Y_predict, average='weighted')\n",
        "Recall = recall_score(Y_actual, Y_predict, average='weighted')\n",
        "F1score = f1_score(Y_actual, Y_predict, average='weighted')\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", Accuracy)\n",
        "print(\"Precision:\", Precision)\n",
        "print(\"Recall:\", Recall)\n",
        "print(\"F1 Score:\", F1score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rloPbFVxaad-"
      },
      "outputs": [],
      "source": [
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train Acc', 'val Acc'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnR3qKvhaahH"
      },
      "outputs": [],
      "source": [
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train-loss', 'val-loss'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkZQ8wmAvZGR"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_val)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4CQom1TwFRu"
      },
      "outputs": [],
      "source": [
        "Y_actual, Y_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwZuCRwIwFWd"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(Y_actual, Y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvV-o4NyqdKU"
      },
      "outputs": [],
      "source": [
        "cm = np.array([[845,6,179,1], [2,935,2,0], [87,5,897,0],[0,0,0,854] ])\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWffPcyVoUUe"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0n0ZjBkpitY"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "plot_confusion_matrix(cm, classes=['AF', 'Normal', 'Other', 'Noise'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKEsYAMrIMbz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}